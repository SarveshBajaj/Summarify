from enum import Enum
from abc import ABC, abstractmethod
from youtube_transcript_api import YouTubeTranscriptApi
from transformers import pipeline
from loguru import logger
import re
from typing import Tuple, List, Dict, Any

# For extensibility
class ProviderType(str, Enum):
    youtube = "youtube"
    # Add more types (web, file, etc) here
    # web = "web"
    # pdf = "pdf"

class ContentProvider(ABC):
    """Base abstract class for all content providers"""

    @abstractmethod
    def get_transcript(self, url: str) -> str:
        """Extract text content from the provided URL"""
        pass

    @abstractmethod
    def summarize_and_validate(self, transcript: str, url: str) -> Tuple[str, bool]:
        """Summarize the transcript and validate the summary"""
        pass

    def _get_summarizer(self):
        """Get the summarization model"""
        logger.info("Loading summarization model")
        return pipeline("summarization", model="facebook/bart-large-cnn")

    def _chunk_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """Split text into chunks of specified size"""
        return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

    def _validate_summary(self, summary: str, transcript: str) -> bool:
        """Validate if the summary is good quality"""
        # Basic validation
        if len(summary) < 100 or len(summary) > 2000:
            return False

        # Check if summary contains key terms from transcript
        # Extract important words from transcript (simple approach)
        words = transcript.lower().split()
        word_freq = {}
        for word in words:
            if len(word) > 4:  # Only consider words longer than 4 chars
                word_freq[word] = word_freq.get(word, 0) + 1

        # Get top 10 words
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        top_words = [word for word, _ in top_words]

        # Check if at least 30% of top words are in summary
        summary_lower = summary.lower()
        matches = sum(1 for word in top_words if word in summary_lower)
        return matches >= len(top_words) * 0.3

def get_provider(provider_type: ProviderType) -> ContentProvider:
    """Factory function to get the appropriate content provider"""
    if provider_type == ProviderType.youtube:
        return YouTubeProvider()
    # Add more providers here as they are implemented
    # if provider_type == ProviderType.web:
    #     return WebProvider()
    # if provider_type == ProviderType.pdf:
    #     return PDFProvider()
    raise NotImplementedError(f"Provider type {provider_type} not implemented")

class YouTubeProvider(ContentProvider):
    """Provider for YouTube video transcripts"""

    def get_transcript(self, url: str) -> str:
        """Get transcript from YouTube video"""
        try:
            video_id = self.extract_video_id(url)
            logger.info(f"Fetching transcript for YouTube video: {video_id}")
            transcript = YouTubeTranscriptApi.get_transcript(video_id)
            text = " ".join([x['text'] for x in transcript])
            logger.info(f"Successfully retrieved transcript ({len(text)} chars)")
            return text
        except Exception as e:
            logger.error(f"Error getting YouTube transcript: {str(e)}")
            raise ValueError(f"Could not get transcript: {str(e)}")

    def summarize_and_validate(self, transcript: str, url: str) -> Tuple[str, bool]:
        """Summarize the transcript and validate the summary"""
        try:
            logger.info(f"Summarizing transcript of length {len(transcript)}")
            summarizer = self._get_summarizer()

            # Target ~1000 words in summary
            # Estimate: each chunk of 1000 chars produces ~100 words of summary
            # So we need ~10 chunks to get 1000 words

            # HuggingFace pipeline has a max token limit, so chunk the text
            chunks = self._chunk_text(transcript, chunk_size=1000)

            # Process each chunk with appropriate length parameters
            summaries = []
            for i, chunk in enumerate(chunks):
                logger.info(f"Processing chunk {i+1}/{len(chunks)}")
                # Adjust max_length based on number of chunks to target ~1000 words total
                max_length = max(100, min(500, 5000 // len(chunks)))
                min_length = max(30, max_length // 4)

                result = summarizer(
                    chunk,
                    max_length=max_length,
                    min_length=min_length,
                    do_sample=False
                )[0]['summary_text']
                summaries.append(result)

            # Combine all summaries
            summary = " ".join(summaries)

            # Validate the summary
            is_okay = self._validate_summary(summary, transcript)

            logger.info(f"Summary generated: {len(summary)} chars, valid: {is_okay}")
            return summary, is_okay

        except Exception as e:
            logger.error(f"Error summarizing content: {str(e)}")
            raise ValueError(f"Summarization failed: {str(e)}")

    def extract_video_id(self, url: str) -> str:
        """Extract YouTube video ID from various URL formats"""
        # More robust patterns for YouTube video IDs
        patterns = [
            r'(?:v=|\/|vi\/|\?v=|\&v=)([\w-]{11})(?:[\?\&]|$)',  # Standard and embedded URLs
            r'(?:youtu\.be\/|youtube\.com\/shorts\/)([\w-]{11})',  # Short URLs and YouTube Shorts
            r'(?:youtube\.com\/embed\/)([\w-]{11})'  # Embed URLs
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        # If we get here, no pattern matched
        logger.error(f"Could not extract YouTube video ID from URL: {url}")
        raise ValueError(f"Invalid YouTube URL format: {url}")
